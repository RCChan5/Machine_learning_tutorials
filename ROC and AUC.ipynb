{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Lets say you want to classify a Yes or No question with logistic regression and some outliers are classified incorrectly with a threshold of .5 you see this and then change the threshold multiple times and make a confusion matrix on each iteration after several iterations and several confusion matrices later you find your self overwhelmed by which one is the best. \n",
    "\n",
    "\n",
    "# ROC Reciever operator characteristic \n",
    "So instead of being overwhelemed by the ammount of confuusion matrices.\n",
    "\n",
    "**Reciever operator characteristic (ROC)** gaphs provide a simple way to summarize all the information\n",
    "![ROC1](images/ROC1.PNG)\n",
    "\n",
    "* Refer to ML fundamental skill showcase for more infor on Sensitivity vs specificity "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read the ROC graph\n",
    "\n",
    "At 1,1 the threshold on the logistic regression is set to 1 this is assuming that everything in the data set is just a yes. in this case we correctly classified ALL of the yes outcome and incorrectly classified all of the no.\n",
    "\n",
    "The diagonal line shows where the true positive rate = false positive rate\n",
    "\n",
    "\n",
    "at 0,0.75 this threshold resulted in no false positives  this means that it correctly classified 75% of the yes sample and 100% of the no\n",
    "\n",
    "**The ROC graph summarizes all of the confusion matricies that each threshold produced**\n",
    "\n",
    "Without sorting the matricies we can immediatly see that the 0,.75 is better thant the .2,.75\n",
    "\n",
    "Depending on how many false positives your willing to accept the optimal threshold is either one of the two circles below\n",
    "![ROC2](images/ROC2.PNG)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC Area under the curve\n",
    "The AUC allows you to compare one ROC curve to another ROC curve\n",
    "![ROC3](images/ROC3.PNG)\n",
    "\n",
    "The red ROC curve is better than the Blue ROC curve So we would use the red method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False positive rate vs Precision \n",
    "\n",
    "Although ROC graphs are drawn using True positice rates and False positive rates to summarize confusion matrices there are other metrics that attempt to do the same \n",
    "\n",
    "For example False positive rate with percision\n",
    "![ROC4](images/ROC4.PNG)\n",
    "\n",
    "**Percision** is the proportion of positive results that were correctly classified\n",
    "* If there were lots of samples that were no relative to the numer that were yes then percision might be more useful than the false positive rate This is because percision does not include the number of True Negative in its calculation and is not effected by the imbalance\n",
    "\n",
    "An example of this imbalance and usage would be when studying a rare disease in this case there are many more people without the disease than with it\n",
    "\n",
    "## Summary\n",
    "\n",
    "ROC curves make it easy to identify the best threshold for making a decision and the AUC can help determine which ML method is better than the other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
